{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Split ECG latent space for Model Poisoning Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Based Per-Class First Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.spatial as sp\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "#from tqdm import tqdm\n",
    "import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from analysis import compute_in_parallel, get_unique_labels, per_label_similarities, get_similarities, filter_labels, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weights classes of a multi-label dataset based on the number of samples in each class. Every unique combination of labels is considered a class. Uses softmax to normalize the weights.\n",
    "def class_weights(dataset):\t\t\n",
    "    # Get the number of samples in each class\n",
    "    class_counts = np.zeros(dataset.num_classes)\n",
    "    for i in range(dataset.num_classes):\n",
    "        class_counts[i] = np.sum(dataset.labels[:, i])\n",
    "    \n",
    "    # Normalize the weights\n",
    "    class_weights = np.exp(class_counts)\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "    \n",
    "    return class_weights\t\n",
    "\n",
    "# Gets a pandas series of 5D numpy arrays. Filters out all arrays, which have a value of 0 in the 3rd dimension.\n",
    "def filter_labels(df, idx, val=1):\t\n",
    "    return df[df.label.apply(lambda x: x[idx] == val)]\n",
    "\n",
    "# Gets a numpy array as input. Returns a numpy array with all possible 2-combinations of the input. The order of the combinations is not important and combinations with the same elements are not included.\n",
    "def get_similarities(X, similarity_functions):\t\n",
    "    similarities = {}\t\n",
    "    for s in similarity_functions:\n",
    "        similarities[s] = sp.distance.pdist(X, s)\n",
    "    return similarities\n",
    "\n",
    "def get_unique_labels(df):\t\n",
    "    decimal = df.label.apply(lambda x: np.sum(x * 2**np.arange(x.size)[::-1]))\n",
    "    ul = decimal.unique()\n",
    "    un = decimal.value_counts().values\n",
    "    ind = np.argsort(ul)\n",
    "    ul = np.take_along_axis(ul, ind, axis=0)\n",
    "    un = np.take_along_axis(un, ind, axis=0)\n",
    "    return np.column_stack((ul, un))\n",
    "\n",
    "def per_label_similarities(group, similarities):\t\n",
    "    latent_vectors = group[1].client_output_pooled.values\n",
    "    latent_vectors = np.array(latent_vectors.tolist())\n",
    "    sim = get_similarities(latent_vectors, similarities)\n",
    "    sim[\"epoch\"] = group[0][0]\n",
    "    sim[\"label\"] = group[0][1]\n",
    "    return pd.DataFrame(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/mohkoh/Projects/Split_ECG_Classification/latent_space/single_client_honest/client_1.pickle\"\n",
    "similarities = [\"cosine\", \"euclidean\", \"cityblock\", \"correlation\", \"jaccard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/mohkoh/Projects/Split_ECG_Classification/latent_space/single_client_honest/client_1.pickle\"\n",
    "similarities = [\"cosine\", \"euclidean\", \"cityblock\", \"correlation\", \"jaccard\"]\n",
    "client1 = pickle.load(open(image_path, \"rb\"))\n",
    "samples = client1[\"samples\"]\n",
    "unique_labels = get_unique_labels(samples)\n",
    "sim_list_per_epoch = pd.DataFrame(columns=[\"epoch\", \"label\"] + similarities)\n",
    "samples_d = samples.copy()\n",
    "samples_d.label = samples_d.label.apply(lambda x: np.sum(x * 2**np.arange(x.size)[::-1]))\n",
    "groups = samples_d.groupby([\"epoch\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(multiprocessing.cpu_count() // 2)\n",
    "r = list(tqdm.tqdm(pool.imap(partial(per_label_similarities, similarities=similarities), iter(groups), len(groups))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "data_outputs = pool.map(partial(per_label_similarities, similarities=similarities), iter(samples_d.groupby([\"epoch\", \"label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EPOCH 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 1/22 [00:05<01:53,  5.40s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "groups = samples_d.groupby([\"epoch\", \"label\"])\n",
    "for i in samples.epoch.unique():\n",
    "    print(\"--------- EPOCH {} ---------\".format(i))\n",
    "    for j in tqdm(unique_labels[:, 0]):\n",
    "        latent_vectors = groups.get_group((i, j)).client_output_pooled.values\n",
    "        latent_vectors = np.array(latent_vectors.tolist())\n",
    "        sim = get_similarities(latent_vectors, similarities)\n",
    "        sim[\"epoch\"] = i\n",
    "        sim[\"label\"] = j\n",
    "        sim_list_per_epoch = pd.concat([sim_list_per_epoch, pd.DataFrame(sim)], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Honest Client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medical-Split-Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc52ed4b74af89c59c87218cbed3cf51503867793a5b3ec27b5161d47e38d1ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
